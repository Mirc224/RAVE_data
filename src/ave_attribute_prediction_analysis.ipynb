{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from evaluation.plotting import plot_metric_for_each_model_grouped_by_size\n",
    "from utility import load_rave_dataset\n",
    "from rave_constants import *\n",
    "import pandas as pd\n",
    "from evaluation.evaluation_utils import read_all_results_in_folder, get_experiment_results_df, evaluate_k_fold, METRICS_COLUMNS\n",
    "from evaluation.experiment_evaluation import *\n",
    "from evaluation.latex_utils import get_latex_table_lines, TableHeaderCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_version = \"v2\"\n",
    "SELECTED_ATTRIBUTES = ALL_SELECTED_ATTRIBUTES_V2 if dataset_version == \"v2\" else ALL_SELECTED_ATTRIBUTES\n",
    "\n",
    "dataset_path = f\"./dataset/rave_dataset_{dataset_version}.json\"\n",
    "rave_dataset = load_rave_dataset(dataset_path)\n",
    "for sample in rave_dataset:\n",
    "    sample.keep_selected_text_attributes(SELECTED_ATTRIBUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file_path = Path(\"./results\")\n",
    "all_results: dict[str, dict] = read_all_results_in_folder(results_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_df_raw = get_experiment_results_df(rave_dataset, all_results, SELECTED_ATTRIBUTES)\n",
    "experiment_code_names = experiment_results_df_raw[ExperimentResults.EXPERIMENT_CODE_NAME].unique().tolist()\n",
    "experiment_code_names_ordered = [\n",
    "    'vanilla',\n",
    "    'oa',\n",
    "    'bow_LogReg',\n",
    "    'bow_SVM',\n",
    "    'bow_XGBoost',\n",
    "    'bow_1-NN',\n",
    "    'bow_3-NN',\n",
    "    'bow_5-NN',\n",
    "    'bow_7-NN',\n",
    "    'embeddings_LogReg',\n",
    "    'embeddings_SVM',\n",
    "    'embeddings_XGBoost',\n",
    "    'embeddings_1-NN',\n",
    "    'embeddings_3-NN',\n",
    "    'embeddings_5-NN',\n",
    "    'embeddings_7-NN',\n",
    "    'tfidf_LogReg',\n",
    "    'tfidf_SVM',\n",
    "    'tfidf_XGBoost',\n",
    "    'tfidf_1-NN',\n",
    "    'tfidf_3-NN',\n",
    "    'tfidf_5-NN',\n",
    "    'tfidf_7-NN',\n",
    "]\n",
    "experiment_results_df_raw[ExperimentResults.EXPERIMENT_CODE_NAME] = pd.Categorical(\n",
    "    experiment_results_df_raw[ExperimentResults.EXPERIMENT_CODE_NAME], \n",
    "    categories=experiment_code_names_ordered, \n",
    "    ordered=True)\n",
    "experiment_results_df = evaluate_k_fold(experiment_results_df_raw)\n",
    "experiment_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_experiment_headers = [\n",
    "    TableHeaderCell('vanilla', can_bold=False),\n",
    "    TableHeaderCell('oa', can_bold=False),\n",
    "    TableHeaderCell('bow_LogReg', \"LogReg\"),\n",
    "    TableHeaderCell('bow_SVM', \"SVM\"),\n",
    "    TableHeaderCell('bow_XGBoost', \"XGBoost\"),\n",
    "    TableHeaderCell('bow_1-NN', \"1-NN\"),\n",
    "    TableHeaderCell('bow_3-NN', \"3-NN\"),\n",
    "    TableHeaderCell('bow_5-NN', \"5-NN\"),\n",
    "    TableHeaderCell('bow_7-NN', \"7-NN\"),\n",
    "]\n",
    "\n",
    "embeddings_experiment_headers = [\n",
    "    TableHeaderCell('vanilla', can_bold=False),\n",
    "    TableHeaderCell('oa', can_bold=False),\n",
    "    TableHeaderCell('embeddings_LogReg','LogReg'),\n",
    "    TableHeaderCell('embeddings_SVM','SVM'),\n",
    "    TableHeaderCell('embeddings_XGBoost','XGBoost'),\n",
    "    TableHeaderCell('embeddings_1-NN','1-NN'),\n",
    "    TableHeaderCell('embeddings_3-NN','3-NN'),\n",
    "    TableHeaderCell('embeddings_5-NN','5-NN'),\n",
    "    TableHeaderCell('embeddings_7-NN','7-NN'),\n",
    "]\n",
    "\n",
    "tfidf_experiment_headers = [\n",
    "    TableHeaderCell('vanilla', can_bold=False),\n",
    "    TableHeaderCell('oa', can_bold=False),\n",
    "    TableHeaderCell('tfidf_LogReg', 'LogReg'),\n",
    "    TableHeaderCell('tfidf_SVM', 'SVM'),\n",
    "    TableHeaderCell('tfidf_XGBoost', 'XGBoost'),\n",
    "    TableHeaderCell('tfidf_1-NN', '1-NN'),\n",
    "    TableHeaderCell('tfidf_3-NN', '3-NN'),\n",
    "    TableHeaderCell('tfidf_5-NN', '5-NN'),\n",
    "    TableHeaderCell('tfidf_7-NN', '7-NN'),\n",
    "]\n",
    "\n",
    "sorted_model_names = experiment_results_df.sort_values(by=[ExperimentResults.MODEL_NAME, ExperimentResults.MODEL_SIZE])[ExperimentResults.MODEL_NAME_W_SIZE].unique()\n",
    "\n",
    "res_bow = get_latex_table_lines(\n",
    "    experiment_results_df, \n",
    "    f\"micro_{ExperimentEvaluator.F1_SCORE}\",\n",
    "    sorted_model_names,\n",
    "    bow_experiment_headers,\n",
    "    \"Micro F1 Score\",\n",
    "    \"BoW\",\n",
    "    \"tab:bow_all_models_micro_f1\",\n",
    "    2,\n",
    "    True)\n",
    "\n",
    "res_tfidf = get_latex_table_lines(\n",
    "    experiment_results_df, \n",
    "    f\"micro_{ExperimentEvaluator.F1_SCORE}\",\n",
    "    sorted_model_names,\n",
    "    tfidf_experiment_headers,\n",
    "    \"Micro F1 Score\",\n",
    "    \"TF-IDF\",\n",
    "    \"tab:tfidf_all_models_micro_f1\",\n",
    "    2,\n",
    "    True)\n",
    "\n",
    "res_embeddings = get_latex_table_lines(\n",
    "    experiment_results_df, \n",
    "    f\"micro_{ExperimentEvaluator.F1_SCORE}\",\n",
    "    sorted_model_names,\n",
    "    embeddings_experiment_headers,\n",
    "    \"Micro F1 Score\",\n",
    "    \"Embeddings\",\n",
    "    \"tab:embeddings_all_models_micro_f1\",\n",
    "    2,\n",
    "    True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_to_experiment(all_data: pd.DataFrame, base_data: pd.DataFrame, experiment_code_name: str) -> pd.DataFrame:\n",
    "    code_name_df = all_data.loc[all_data[ExperimentResults.EXPERIMENT_CODE_NAME] == experiment_code_name, [ExperimentResults.MODEL_NAME_W_SIZE, *METRICS_COLUMNS]]\n",
    "    result_df = pd.merge(base_data, code_name_df, on=ExperimentResults.MODEL_NAME_W_SIZE, suffixes=(\"\", f\"_{experiment_code_name}\"))\n",
    "    delta_suffix = \"_delta\"\n",
    "    for metric_name in METRICS_COLUMNS:\n",
    "        result_df[f\"{metric_name}_{experiment_code_name}{delta_suffix}\"] = result_df[metric_name] - result_df[f\"{metric_name}_{experiment_code_name}\"]\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_oa_df = experiment_results_df.loc[experiment_results_df[ExperimentResults.ORACLE_ATTRIBUTES] == False]\n",
    "idx = no_oa_df.groupby(ExperimentResults.MODEL_NAME_W_SIZE)[f\"micro_{ExperimentEvaluator.F1_SCORE}\"].idxmax()\n",
    "best_no_oa_df = experiment_results_df.loc[idx]\n",
    "differneces_df = get_delta_to_experiment(experiment_results_df, best_no_oa_df, \"oa\")\n",
    "differneces_df = get_delta_to_experiment(experiment_results_df, differneces_df, \"vanilla\")\n",
    "differneces_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_micro_f1_score_for_each_model_all_settings(experiment_results_df: pd.DataFrame, take_max:bool):\n",
    "    correct_text = \"Highest\" if take_max else \"Lowest\"\n",
    "    plot_metric_for_each_model_grouped_by_size(\n",
    "        experiment_results_df, \n",
    "        f\"micro_{ExperimentEvaluator.F1_SCORE}\", \n",
    "        \"Micro F1 Score (%)\", \n",
    "        f\"{correct_text} micro F1 score achieved\", \n",
    "        take_max,\n",
    "        2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_micro_f1_score_for_each_model_all_settings(differneces_df, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
